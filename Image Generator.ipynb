{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlus\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "file_name = \"example.jpg\" # Put in actual file name here\n",
    "image = cv2.imread(filename=file_name)\n",
    "faces = app.get(image)\n",
    "\n",
    "faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n",
    "face_image = face_align.norm_crop(image, landmark=faces[0].kps, image_size=224) # you can also segment the face\n",
    "v2 = False\n",
    "base_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = \"ip-adapter-faceid-plus_sd15.bin\" if not v2 else \"ip-adapter-faceid-plusv2_sd15.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "# load ip-adapter\n",
    "ip_model = IPAdapterFaceIDPlus(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "# generate image\n",
    "prompt = \"Scientist Tom makes a significant discovery in biology lab.\" # This is an example prompt. Enter Prompt here\n",
    "negative_prompt = \"multiple hands, deformed fingers, monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n",
    "\n",
    "images = ip_model.generate(\n",
    "     prompt=prompt, negative_prompt=negative_prompt, face_image=face_image, faceid_embeds=faceid_embeds, shortcut=v2, s_scale=1.0,\n",
    "     num_samples=1, width=512, height=768, num_inference_steps=35, seed=2023, guidance_scale=8\n",
    ")\n",
    "i = 0;\n",
    "for image in images:\n",
    "    image.show()\n",
    "    image.save(f\"output_{i}_out.png\")\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
