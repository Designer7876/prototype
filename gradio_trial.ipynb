{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.16 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:05<00:00,  1.12s/it]\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\lora.py:208: FutureWarning: `LoRALinearLayer` is deprecated and will be removed in version 1.0.0. Use of `LoRALinearLayer` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRALinearLayer\", \"1.0.0\", deprecation_message)\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ip_adapter\\ip_adapter_faceid.py:319: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlus\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "import subprocess\n",
    "\n",
    "# Initialize models and face analysis\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "v2 = False\n",
    "base_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = \"ip-adapter-faceid-plus_sd15.bin\" if not v2 else \"ip-adapter-faceid-plusv2_sd15.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "# Load IP-Adapter\n",
    "ip_model = IPAdapterFaceIDPlus(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "# Function to generate text prompts using Mistral\n",
    "def generate_text_with_mistral(prompt):\n",
    "    try:\n",
    "        structured_prompt = (\n",
    "            f\"{prompt}\\n\\n\"\n",
    "            \"Please summarize this story in exactly 4 concise and coherent sentences. \"\n",
    "            \"Do not include any additional text.\"\n",
    "        )\n",
    "        \n",
    "        command = [\"ollama\", \"run\", \"mistral-nemo\"]\n",
    "        result = subprocess.run(\n",
    "            command, input=structured_prompt,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            raise Exception(f\"Error generating text: {result.stderr}\")\n",
    "        \n",
    "        output = result.stdout.strip().replace('\\n', ' ')\n",
    "        sentences = [s.strip() for s in output.replace(';', '.').split('. ') if s.strip()]\n",
    "\n",
    "        summarized_sentences = sentences[:4]\n",
    "        return summarized_sentences\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate images based on the story and image\n",
    "def generate_images_from_story(image_file, story_text):\n",
    "    # Load and process the face image\n",
    "    image = cv2.imread(image_file)\n",
    "    faces = app.get(image)\n",
    "\n",
    "    faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n",
    "    face_image = face_align.norm_crop(image, landmark=faces[0].kps, image_size=224)\n",
    "\n",
    "    # Step 1: Generate 4 prompts from the story\n",
    "    summarized_prompts = generate_text_with_mistral(story_text)\n",
    "    \n",
    "    if not summarized_prompts:\n",
    "        return [None, None, None, None]\n",
    "\n",
    "    negative_prompt = \"multiple hands, deformed fingers, monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n",
    "    \n",
    "    # Step 2: Generate 4 images based on the prompts\n",
    "    generated_images = []\n",
    "    for i, prompt in enumerate(summarized_prompts):\n",
    "        images = ip_model.generate(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            face_image=face_image,\n",
    "            faceid_embeds=faceid_embeds,\n",
    "            shortcut=v2,\n",
    "            s_scale=1.0,\n",
    "            num_samples=1,\n",
    "            width=512,\n",
    "            height=768,\n",
    "            num_inference_steps=35,\n",
    "            seed=2023,\n",
    "            guidance_scale=8\n",
    "        )\n",
    "        # Append generated image\n",
    "        for img in images:\n",
    "            generated_images.append(img)\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "# Gradio interface function\n",
    "def gradio_interface(image, story):\n",
    "    generated_images = generate_images_from_story(image, story)\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "# Gradio app setup\n",
    "gr_interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[gr.Image(type=\"filepath\"), gr.Textbox(lines=10, placeholder=\"Enter your story here...\")],\n",
    "    outputs=[gr.Image(label=f\"Generated Image {i+1}\") for i in range(4)],\n",
    "    title=\"Story-to-Image Generator with Face Integration\",\n",
    "    description=\"Upload an image and enter a story. The app will generate images based on the story and integrate the face from the uploaded image.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.16 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:05<00:00,  1.13s/it]\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\lora.py:208: FutureWarning: `LoRALinearLayer` is deprecated and will be removed in version 1.0.0. Use of `LoRALinearLayer` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRALinearLayer\", \"1.0.0\", deprecation_message)\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ip_adapter\\ip_adapter_faceid.py:319: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "Exception in thread Thread-12 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Edjon\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "  File \"c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x8f in position 200: character maps to <undefined>\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:480: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 35/35 [00:08<00:00,  4.23it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\blocks.py\", line 1945, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\blocks.py\", line 1717, in postprocess_data\n",
      "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\blocks.py\", line 1691, in validate_outputs\n",
      "    raise ValueError(\n",
      "ValueError: An event handler (gradio_interface) didn't receive enough output values (needed: 4, received: 1).\n",
      "Wanted outputs:\n",
      "    [<gradio.components.image.Image object at 0x000001F7F53873B0>, <gradio.components.image.Image object at 0x000001F817F138C0>, <gradio.components.image.Image object at 0x000001F817F133B0>, <gradio.components.image.Image object at 0x000001F7F9EBF380>]\n",
      "Received outputs:\n",
      "    [\"generated_image_0_0.png\"]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlus\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Initialize models and face analysis\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "v2 = False\n",
    "base_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = \"ip-adapter-faceid-plus_sd15.bin\" if not v2 else \"ip-adapter-faceid-plusv2_sd15.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "# Load IP-Adapter\n",
    "ip_model = IPAdapterFaceIDPlus(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "# Function to generate text prompts using Mistral\n",
    "def generate_text_with_mistral(prompt):\n",
    "    try:\n",
    "        structured_prompt = (\n",
    "            f\"{prompt}\\n\\n\"\n",
    "            \"Please summarize and break this story about a scientist flow-wise in exactly 4 concise and coherent sentences. Each sentence should have a maximum of 12 words and must be scientific and in simple but scientific words. These sentences must be prompts for stable diffusion to generate images.\"\n",
    "            \"Do not include any additional text.\"\n",
    "        )\n",
    "        \n",
    "        command = [\"ollama\", \"run\", \"mistral-nemo\"]\n",
    "        result = subprocess.run(\n",
    "            command, input=structured_prompt,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            raise Exception(f\"Error generating text: {result.stderr}\")\n",
    "        \n",
    "        output = result.stdout.strip().replace('\\n', ' ')\n",
    "        sentences = [s.strip() for s in output.replace(';', '.').split('. ') if s.strip()]\n",
    "\n",
    "        summarized_sentences = sentences[:4]\n",
    "        return summarized_sentences\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate images based on the story and image\n",
    "def generate_images_from_story(image_file, story_text):\n",
    "    # Load and process the face image\n",
    "    image = cv2.imread(image_file)\n",
    "    faces = app.get(image)\n",
    "\n",
    "    faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n",
    "    face_image = face_align.norm_crop(image, landmark=faces[0].kps, image_size=224)\n",
    "\n",
    "    # Step 1: Generate 4 prompts from the story\n",
    "    summarized_prompts = generate_text_with_mistral(story_text)\n",
    "    \n",
    "    if not summarized_prompts:\n",
    "        return [None, None, None, None]\n",
    "\n",
    "    negative_prompt = \"(multiple faces), (muliple people), multiple hands, deformed fingers, monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n",
    "    \n",
    "    # Step 2: Generate 4 images based on the prompts and save them to file paths\n",
    "    generated_images_paths = []\n",
    "    for i, prompt in enumerate(summarized_prompts):\n",
    "        images = ip_model.generate(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            face_image=face_image,\n",
    "            faceid_embeds=faceid_embeds,\n",
    "            shortcut=v2,\n",
    "            s_scale=1.0,\n",
    "            num_samples=1,\n",
    "            width=512,\n",
    "            height=768,\n",
    "            num_inference_steps=35,\n",
    "            seed=2023,\n",
    "            guidance_scale=8\n",
    "        )\n",
    "        \n",
    "        # Save the generated images to files and return the file paths\n",
    "        for j, img in enumerate(images):\n",
    "            img_path = f\"generated_image_{i}_{j}.png\"\n",
    "            img.save(img_path)\n",
    "            generated_images_paths.append(img_path)\n",
    "    \n",
    "    return generated_images_paths\n",
    "\n",
    "# Gradio interface function\n",
    "def gradio_interface(image, story):\n",
    "    generated_image_paths = generate_images_from_story(image, story)\n",
    "    return generated_image_paths\n",
    "\n",
    "# Gradio app setup\n",
    "gr_interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[gr.Image(type=\"filepath\"), gr.Textbox(lines=10, placeholder=\"Enter your story here...\")],\n",
    "    outputs=[gr.Image(label=f\"Generated Image {i+1}\") for i in range(4)],\n",
    "    title=\"Story-to-Image Generator with Face Integration\",\n",
    "    description=\"Upload an image and enter a story. The app will generate images based on the story and integrate the face from the uploaded image.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "# def embedding(image_paths, prompt):\n",
    "def embed_text_as_image_novel(image_path, text):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Set up the font and size\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 16)  # You can customize the font and size\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Wrap the text to fit the image width\n",
    "    max_width = image.width - 20  # Leave some padding\n",
    "    wrapped_text = textwrap.fill(text, width=40)  # Adjust width as needed\n",
    "\n",
    "    # Calculate text size using textbbox\n",
    "    text_bbox = draw.textbbox((0, 0), wrapped_text, font=font)\n",
    "    text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n",
    "\n",
    "    # Calculate text position at the bottom of the image\n",
    "    padding = 10\n",
    "    text_position = (10, image.height - text_height - padding)  # Adjust padding if needed\n",
    "\n",
    "    # Draw a rectangle behind the text for better visibility (optional)\n",
    "    rectangle_bbox = [text_position[0] - 5, text_position[1] - 5, \n",
    "                    text_position[0] + text_width + 5, text_position[1] + text_height + 5]\n",
    "    draw.rectangle(rectangle_bbox, fill=\"black\")\n",
    "\n",
    "    # Draw the text on the image\n",
    "    draw.text(text_position, wrapped_text, font=font, fill=\"white\")\n",
    "\n",
    "    # Save the image with the embedded text\n",
    "    output_path = \"output_\" + image_path.split('/')[-1]  # Prepend 'output_' to the filename\n",
    "    image.save(output_path)\n",
    "    return output_path\n",
    "\n",
    "def display_image(image_path):\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "def generate_text_with_mistral(prompt, image_path):\n",
    "    try:\n",
    "        # Call the CLI command, adjust as per the actual command structure\n",
    "        command = [\"ollama\", \"run\", \"mistral-nemo\", image_path]\n",
    "        result = subprocess.run(\n",
    "            command, input=prompt,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        # Check for errors\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(f\"Error generating text: {result.stderr}\")\n",
    "        \n",
    "        return result.stdout.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_images_sequentially(image_paths, prompt):\n",
    "    comic_data = []\n",
    "    \n",
    "# for i, image_path in enumerate(image_paths):\n",
    "    # Generate the dialogue for each image sequentially using the same prompt\n",
    "    generated_text = generate_text_with_mistral(prompt, image_paths)\n",
    "    \n",
    "    # Store the image and its corresponding dialogue in the dictionary\n",
    "    \n",
    "    \n",
    "    # Embed the dialogue into the image\n",
    "    output_image_path = embed_text_as_image_novel(image_paths, generated_text)\n",
    "    comic_data.append(output_image_path)\n",
    "    print(f\"Generated Text for {image_paths}: {generated_text}\")\n",
    "    print(f\"Output Image Path: {output_image_path}\")\n",
    "\n",
    "    # Display the image\n",
    "    display_image(output_image_path)\n",
    "\n",
    "        # return comic_data\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "desc = \"Scientist Tom makes a significant discovery in biology lab.\"\n",
    "\n",
    "prompt = f\"I want you to follow the scientific description {desc} for the image i shall provide. I want you to Generate JUST ONE small dialogue WITH THE PERSON SPEAKING STORY WISE for image and no other text.\"\n",
    "\n",
    "process_images_sequentially(\"output.png\", prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final App Trial (Still In Progress, but almost done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.16 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:06<00:00,  1.32s/it]\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\lora.py:208: FutureWarning: `LoRALinearLayer` is deprecated and will be removed in version 1.0.0. Use of `LoRALinearLayer` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRALinearLayer\", \"1.0.0\", deprecation_message)\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ip_adapter\\ip_adapter_faceid.py:319: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlus\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "import subprocess\n",
    "import os\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "v2 = False\n",
    "base_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = \"ip-adapter-faceid-plus_sd15.bin\" if not v2 else \"ip-adapter-faceid-plusv2_sd15.bin\"\n",
    "device = \"cuda\"\n",
    "story_summary = None\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "# Load IP-Adapter\n",
    "ip_model = IPAdapterFaceIDPlus(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "# First Mistral call for generating story prompts\n",
    "def generate_story_prompts(story_text):\n",
    "    try:\n",
    "        # Structured prompt for Mistral to summarize the story into 4 sequential prompts\n",
    "        structured_prompt = (\n",
    "            f\"{story_text}\\n\\n\"\n",
    "            \"Please summarize and break this story about a scientist flow-wise in exactly 4 concise and coherent sentences. Each sentence should have a maximum of 15 words and must be scientific and in simple words.\"\n",
    "            \"Do not include any additional text.\"\n",
    "        )\n",
    "\n",
    "        # Call Mistral Nemo to summarize the story into 4 prompts\n",
    "        command = [\"ollama\", \"run\", \"mistral-nemo\"]\n",
    "        result = subprocess.run(\n",
    "            command, input=structured_prompt,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(f\"Error generating story prompts: {result.stderr}\")\n",
    "\n",
    "        output = result.stdout.strip()\n",
    "        story_summary = output\n",
    "        sentences = [s.strip() for s in output.replace(';', '.').split('. ') if s.strip()]\n",
    "        return sentences[:4]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating story prompts: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Second Mistral call for generating dialogue for each image\n",
    "def generate_comic_dialogue(prompt, image_path, story_text):\n",
    "    try:\n",
    "        # Structured prompt for Mistral to generate dialogue based on the image and story\n",
    "        structured_prompt = (\n",
    "            f\"Story: {story_text}\\n\\n\"\n",
    "            f\"Part: {prompt}\\n\\n\"\n",
    "            \"Generate JUST ONE short scientific dialogue for the person in this image, following the story and part flow. ONLY THE PERSON PRESENT IN IMAGE MUST HAVE DIALOGUE. No other text.\"\n",
    "        )\n",
    "\n",
    "        command = [\"ollama\", \"run\", \"mistral-nemo\", image_path]\n",
    "        result = subprocess.run(\n",
    "            command, input=structured_prompt,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(f\"Error generating dialogue: {result.stderr}\")\n",
    "\n",
    "        return result.stdout.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating comic dialogue: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Embed text into image\n",
    "def embed_text_as_image_novel(image_path, text):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    max_width = image.width - 20\n",
    "    wrapped_text = textwrap.fill(text, width=40)\n",
    "\n",
    "    text_bbox = draw.textbbox((0, 0), wrapped_text, font=font)\n",
    "    text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n",
    "\n",
    "    padding = 10\n",
    "    text_position = (10, image.height - text_height - padding)\n",
    "\n",
    "    rectangle_bbox = [text_position[0] - 5, text_position[1] - 5, \n",
    "                    text_position[0] + text_width + 5, text_position[1] + text_height + 5]\n",
    "    draw.rectangle(rectangle_bbox, fill=\"black\")\n",
    "    draw.text(text_position, wrapped_text, font=font, fill=\"white\")\n",
    "\n",
    "    output_path = \"output_\" + image_path.split('/')[-1]\n",
    "    image.save(output_path)\n",
    "    return output_path\n",
    "\n",
    "# Process each image to generate dialogue and embed it\n",
    "def process_images_sequentially(image_paths, prompts, story_text):\n",
    "    comic_data = []\n",
    "\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # Call the second Mistral function to generate dialogue for each image\n",
    "        generated_text = generate_comic_dialogue(prompts[i], image_path, story_text)\n",
    "        output_image_path = embed_text_as_image_novel(image_path, generated_text)\n",
    "        comic_data.append(output_image_path)\n",
    "        print(f\"Processed image {i+1}/{len(image_paths)}: Dialogue: {generated_text}\")\n",
    "\n",
    "    return comic_data\n",
    "\n",
    "# Create comic storyboard from processed images\n",
    "def create_comic_storyboard(image_paths, output_path, grid_size=(3, 2), padding=10, background_color=(255, 255, 255)):\n",
    "    images = [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]\n",
    "\n",
    "    image_width, image_height = images[0].size\n",
    "    total_width = grid_size[1] * image_width + (grid_size[1] - 1) * padding\n",
    "    total_height = grid_size[0] * image_height + (grid_size[0] - 1) * padding\n",
    "\n",
    "    storyboard = Image.new('RGB', (total_width, total_height), color=background_color)\n",
    "\n",
    "    for index, image in enumerate(images):\n",
    "        row = index // grid_size[1]\n",
    "        col = index % grid_size[1]\n",
    "        x_offset = col * (image_width + padding)\n",
    "        y_offset = row * (image_height + padding)\n",
    "        storyboard.paste(image, (x_offset, y_offset))\n",
    "\n",
    "    storyboard.save(output_path)\n",
    "    return output_path\n",
    "\n",
    "# Function to generate images based on the story and image\n",
    "def generate_images_from_story(image_file, story_text):\n",
    "    image = cv2.imread(image_file)\n",
    "    faces = app.get(image)\n",
    "\n",
    "    faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n",
    "    face_image = face_align.norm_crop(image, landmark=faces[0].kps, image_size=224)\n",
    "\n",
    "    # First, generate 4 story prompts using Mistral\n",
    "    summarized_prompts = generate_story_prompts(story_text)\n",
    "\n",
    "    if not summarized_prompts:\n",
    "        return [None, None, None, None]\n",
    "\n",
    "    negative_prompt = \"multiple faces, multiple hands, deformed fingers, monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n",
    "    \n",
    "    # Generate images based on the prompts\n",
    "    generated_images_paths = []\n",
    "    for i, prompt in enumerate(summarized_prompts):\n",
    "        images = ip_model.generate(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            face_image=face_image,\n",
    "            faceid_embeds=faceid_embeds,\n",
    "            shortcut=v2,\n",
    "            s_scale=1.0,\n",
    "            num_samples=1,\n",
    "            width=512,\n",
    "            height=768,\n",
    "            num_inference_steps=35,\n",
    "            seed=2023,\n",
    "            guidance_scale=8\n",
    "        )\n",
    "        \n",
    "        for j, img in enumerate(images):\n",
    "            img_path = f\"generated_image_{i}_{j}.png\"\n",
    "            img.save(img_path)\n",
    "            generated_images_paths.append(img_path)\n",
    "    \n",
    "    return generated_images_paths, summarized_prompts\n",
    "\n",
    "# Gradio interface function\n",
    "def gradio_interface(image, story):\n",
    "    generated_image_paths, prompts = generate_images_from_story(image, story)\n",
    "\n",
    "    # Process each image with dialogue embedding\n",
    "    processed_images = process_images_sequentially(generated_image_paths, prompts, story_summary)\n",
    "\n",
    "    # Create comic storyboard\n",
    "    storyboard_path = create_comic_storyboard(processed_images, \"comic_storyboard.png\", grid_size=(2, 2))\n",
    "    \n",
    "    return storyboard_path\n",
    "\n",
    "# Gradio app setup\n",
    "gr_interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[gr.Image(type=\"filepath\"), gr.Textbox(lines=10, placeholder=\"Enter your story here...\")],\n",
    "    outputs=gr.Image(label=\"Generated Comic Storyboard\"),\n",
    "    title=\"Story-to-Image Comic Generator\",\n",
    "    description=\"Upload an image and enter a story. The app will generate comic images based on the story, integrate the face from the uploaded image, and create a comic storyboard.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.16 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "c:\\Users\\Edjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Edjon/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  40%|████      | 2/5 [00:00<00:01,  2.28it/s]"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlus\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "import subprocess\n",
    "import os\n",
    "import textwrap\n",
    "import gc  # Garbage collection for memory management\n",
    "\n",
    "# Initialize models and face analysis\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "v2 = False\n",
    "base_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = \"ip-adapter-faceid-plus_sd15.bin\" if not v2 else \"ip-adapter-faceid-plusv2_sd15.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "# Load IP-Adapter\n",
    "ip_model = IPAdapterFaceIDPlus(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "# First Mistral call for generating story prompts\n",
    "def generate_story_prompts(story_text):\n",
    "    try:\n",
    "        # Structured prompt for Mistral to summarize the story into 4 sequential prompts\n",
    "        structured_prompt = (\n",
    "            f\"{story_text}\\n\\n\"\n",
    "            \"Please summarize and break this story about a scientist flow-wise in exactly 4 concise and coherent sentences. Each sentence should have a maximum of 15 words and must be scientific and in simple words.\"\n",
    "            \"Do not include any additional text.\"\n",
    "        )\n",
    "\n",
    "        # Call Mistral Nemo to summarize the story into 4 prompts\n",
    "        command = [\"ollama\", \"run\", \"mistral-nemo\"]\n",
    "        result = subprocess.run(\n",
    "            command, input=structured_prompt,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(f\"Error generating story prompts: {result.stderr}\")\n",
    "\n",
    "        output = result.stdout.strip()\n",
    "        sentences = [s.strip() for s in output.replace(';', '.').split('. ') if s.strip()]\n",
    "        return sentences[:4]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating story prompts: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Second Mistral call for generating dialogue for each image\n",
    "def generate_comic_dialogue(prompt, image_path):\n",
    "    try:\n",
    "        # Structured prompt for Mistral to generate dialogue based on the image and story\n",
    "        structured_prompt = (\n",
    "            f\"{prompt}\\n\\n\"\n",
    "            \"Generate JUST ONE short scientific dialogue for the person in this image, following the story flow. ONLY THE PERSON PRESENT IN IMAGE MUST HAVE DIALOGUE. No other text.\"\n",
    "        )\n",
    "\n",
    "        command = [\"ollama\", \"run\", \"mistral-nemo\", image_path]\n",
    "        result = subprocess.run(\n",
    "            command, input=structured_prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=60  # Timeout to avoid hanging\n",
    "        )\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(f\"Error generating dialogue: {result.stderr}\")\n",
    "\n",
    "        return result.stdout.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating comic dialogue: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Embed text into image\n",
    "def embed_text_as_image_novel(image_path, text):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    max_width = image.width - 20\n",
    "    wrapped_text = textwrap.fill(text, width=40)\n",
    "\n",
    "    text_bbox = draw.textbbox((0, 0), wrapped_text, font=font)\n",
    "    text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n",
    "\n",
    "    padding = 10\n",
    "    text_position = (10, image.height - text_height - padding)\n",
    "\n",
    "    rectangle_bbox = [text_position[0] - 5, text_position[1] - 5, \n",
    "                    text_position[0] + text_width + 5, text_position[1] + text_height + 5]\n",
    "    draw.rectangle(rectangle_bbox, fill=\"black\")\n",
    "    draw.text(text_position, wrapped_text, font=font, fill=\"white\")\n",
    "\n",
    "    output_path = \"output_\" + image_path.split('/')[-1]\n",
    "    image.save(output_path)\n",
    "    return output_path\n",
    "\n",
    "# Function to generate images based on the story and image\n",
    "def generate_images_from_story(image_file, story_text):\n",
    "    image = cv2.imread(image_file)\n",
    "    faces = app.get(image)\n",
    "\n",
    "    faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n",
    "    face_image = face_align.norm_crop(image, landmark=faces[0].kps, image_size=224)\n",
    "\n",
    "    # First, generate 4 story prompts using Mistral\n",
    "    summarized_prompts = generate_story_prompts(story_text)\n",
    "\n",
    "    if not summarized_prompts:\n",
    "        return [None, None, None, None]\n",
    "\n",
    "    negative_prompt = \"multiple faces, multiple hands, deformed fingers, monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n",
    "    \n",
    "    # Generate images based on the prompts\n",
    "    generated_images_paths = []\n",
    "    for i, prompt in enumerate(summarized_prompts):\n",
    "        images = ip_model.generate(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            face_image=face_image,\n",
    "            faceid_embeds=faceid_embeds,\n",
    "            shortcut=v2,\n",
    "            s_scale=1.0,\n",
    "            num_samples=1,\n",
    "            width=512,\n",
    "            height=768,\n",
    "            num_inference_steps=35,\n",
    "            seed=2023,\n",
    "            guidance_scale=8\n",
    "        )\n",
    "        \n",
    "        # Save the generated images to files and return the file paths\n",
    "        for j, img in enumerate(images):\n",
    "            img_path = f\"generated_image_{i}_{j}.png\"\n",
    "            img.save(img_path)\n",
    "            generated_images_paths.append(img_path)\n",
    "    \n",
    "    return generated_images_paths, summarized_prompts\n",
    "\n",
    "# Process each image to generate dialogue and embed it\n",
    "def process_images_sequentially(image_paths, prompts):\n",
    "    comic_data = []\n",
    "\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # Call the second Mistral function to generate dialogue for each image\n",
    "        generated_text = generate_comic_dialogue(prompts[i], image_path)\n",
    "        output_image_path = embed_text_as_image_novel(image_path, generated_text)\n",
    "        comic_data.append(output_image_path)\n",
    "\n",
    "        # Release resources and free up memory for each image after processing\n",
    "        del generated_text\n",
    "        del image_path\n",
    "        gc.collect()\n",
    "\n",
    "    return comic_data\n",
    "\n",
    "# Create comic storyboard from processed images\n",
    "def create_comic_storyboard(image_paths, output_path, grid_size=(2, 2), padding=10, background_color=(255, 255, 255)):\n",
    "    images = [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]\n",
    "\n",
    "    image_width, image_height = images[0].size\n",
    "    total_width = grid_size[1] * image_width + (grid_size[1] - 1) * padding\n",
    "    total_height = grid_size[0] * image_height + (grid_size[0] - 1) * padding\n",
    "\n",
    "    storyboard = Image.new('RGB', (total_width, total_height), color=background_color)\n",
    "\n",
    "    for index, image in enumerate(images):\n",
    "        row = index // grid_size[1]\n",
    "        col = index % grid_size[1]\n",
    "        x_offset = col * (image_width + padding)\n",
    "        y_offset = row * (image_height + padding)\n",
    "        storyboard.paste(image, (x_offset, y_offset))\n",
    "\n",
    "    storyboard.save(output_path)\n",
    "    return output_path\n",
    "\n",
    "# Gradio interface function\n",
    "def gradio_interface(image, story):\n",
    "    generated_image_paths, prompts = generate_images_from_story(image, story)\n",
    "\n",
    "    # Process each image with dialogue embedding\n",
    "    processed_images = process_images_sequentially(generated_image_paths, prompts)\n",
    "\n",
    "    # Create comic storyboard\n",
    "    storyboard_path = create_comic_storyboard(processed_images, \"comic_storyboard.png\", grid_size=(2, 2))\n",
    "    \n",
    "    return storyboard_path\n",
    "\n",
    "# Gradio app setup\n",
    "gr_interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[gr.Image(type=\"filepath\"), gr.Textbox(lines=10, placeholder=\"Enter your story here...\")],\n",
    "    outputs=gr.Image(label=\"Generated Comic Storyboard\"),\n",
    "    title=\"Story-to-Image Comic Generator\",\n",
    "    description=\"Upload an image and enter a story. The app will generate comic images based on the story, integrate the face from the uploaded image, and create a comic storyboard.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
